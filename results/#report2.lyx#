#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 0.7in
\topmargin 1in
\rightmargin 0.7in
\bottommargin 1in
\columnsep 0.1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Mini-project Report
\end_layout

\begin_layout Author
Samuel Humeau, Benyounes Moumni
\end_layout

\begin_layout Abstract
The goal of this project is to implement and train a multilayer perceptron
 to classify images from the norb dataset.
 The dataset has XXXX images of object in 5 different categories: four-legged
 animals, human figures, airplanes, trucks, and cars.
 The Train set contains YYY samples while the test set contains XXX.
 In Section 2 we describe the implementations and result for the first task
 (binary classification.
 Section 3 and 4 describe the implementation of the mlp for multi way classifica
tion.
 We conclude in section 5.
\end_layout

\begin_layout Section
Binary Classification
\end_layout

\begin_layout Standard
The first task is to implement a binary classifier using a 2-hidden layer
 perceptron with logistic error.
 the architecture of the network is described in CITATION.
 We will first describe how we derived the gradient formulas then show the
 results.
 
\end_layout

\begin_layout Subsection
Normalization
\end_layout

\begin_layout Standard
We use the normalization procedure as described in the project guidelines.
 We normalize the validation set and save the mean and variance.
 These values are used to normalize the validation set and test set.
 
\end_layout

\begin_layout Subsection
Early Stopping
\end_layout

\begin_layout Standard
After observing the evolution of the validation error after each epoch,
 we noticed that stopping after the validation error start increasing is
 not a good idea.
 Indeed the curve is highly variable during the first epochs.
 Instead we average the validation error over 4 epochs, that way, small
 fluctualtion will be ignored.
 Also given that validation error rarely goes up we decided to stop when
 the improvement becomes unsignificant to speed up the training time i.e.
 
\begin_inset Formula $error(epoch-3:epoch)>0.95\times error(epoch-7:epoch-4)$
\end_inset

.
\end_layout

\begin_layout Subsection
Results 
\end_layout

\begin_layout Standard
We tried the classifier with several configuration of number of layers and
 learning rate.
 The classification in all cases is perfect after the first epoch with a
 very low error.
 The reason for such a good performance on the validation set is that the
 training set is very well separated.
 In this case there is not much optimization possible, as the difference
 in error in the validation is insignificant to make a decision.
\end_layout

\begin_layout Subsubsection
Extrapolation ?
\end_layout

\begin_layout Standard
So to check the algorithm beyond testing the gradient we made another dataset
 from the 5class one.
 The idea is that some other classes are less separable and would allow
 us to have a better idea of the performance of our implementation.
\end_layout

\begin_layout Section
Multi-way classification
\end_layout

\begin_layout Standard
To be able to compare the multiway MLP were going to implement, it is important
 to build other solutions to compare.
 Indeed it is expected that a MLP outperforms all linear solutions.
 Therefore we have implemented 3 linear classifiers described in the next
 three sections.
 
\end_layout

\begin_layout Subsection
Linear regression with squared error
\end_layout

\begin_layout Standard
For this part, each datapoint belongs to one of 5 class.
 Thus, the label attributed for each datapoint is no more a scalar but a
 binary 5-dimensional vector.
 For example if 
\begin_inset Formula $x$
\end_inset

 belongs to the class number 2, its label will be 
\begin_inset Formula $\widetilde{t_{i}}=[0,1,0,0,0]$
\end_inset

.
 Let consider the classification error on the training set to be : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{2}(W)=\sum_{i=1}^{N}\left\Vert y_{i}-\tilde{t}_{i}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The linear classifier computes the 5-dimensional output : 
\begin_inset Formula $y_{i}=W[x_{i};1]$
\end_inset

 (we had a constant coordinate in each data points in order to incorporate
 the bias in the matrix W).
 Then the class is determined by 
\begin_inset Formula $argmax_{j}y_{i}(j)$
\end_inset

.
 In the case of the squared error above, the course shows that the matrix
 W that optimizes the regression on the training set is given by : 
\end_layout

\begin_layout Standard
\begin_inset Formula $W_{optimal}=(\Phi^{T}\Phi)^{-1}\Phi T$
\end_inset

 where 
\begin_inset Formula $\Phi$
\end_inset

is a matrix where each column is a datapoint, and 
\begin_inset Formula $T$
\end_inset

 is the the matrix that regroup the corresponding labels.
 
\end_layout

\begin_layout Standard
By computing this classifier, we obtain 1228 misclassified elements on the
 test set (over a total of 5400 data points), which means an accuracy of
 77%.
 
\end_layout

\begin_layout Subsection
Squared error with Tichonov regularizer
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:section_tichonov"

\end_inset


\end_layout

\begin_layout Standard
A classic improvement of linear regression is to constrain the weights using
 a tichonov regularizer.
 The error is now : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{2}(W)=\frac{1}{2}\sum_{i=1}^{N}\left\Vert y_{i}-\tilde{t}_{i}\right\Vert ^{2}+\frac{\nu}{2}\left\Vert W\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
In this case, for a fixed 
\begin_inset Formula $\nu$
\end_inset

, the course gives us the optimal weights for the regression on the training
 set (using the same notation introduced above) :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{tichonov}=(\Phi^{T}\Phi+\nu I)^{-1}\Phi^{T}T
\]

\end_inset

In order to find the best regularizer 
\begin_inset Formula $\nu$
\end_inset

, we compute a 10-folds cross validation on the training set, for different
 values of 
\begin_inset Formula $\nu$
\end_inset

.
 We have selected the regularizer that minimizes the risk, which is computed
 as follow : after splitting the training set in 10 equals parts, 
\begin_inset Formula $risk=\frac{1}{10}\sum_{m=1}^{10}R^{(-m)}(W^{(-m)})$
\end_inset

, where 
\begin_inset Formula $R^{(-m)}$
\end_inset

is the error 
\begin_inset Formula $E_{2}$
\end_inset

computed only of the 
\begin_inset Formula $m^{th}$
\end_inset

part of the classifier trained with the 9 other parts of the training set.
 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tichonov_optimization"

\end_inset

 shows the evolution of the risk with 
\begin_inset Formula $\nu$
\end_inset

.
 Since minimum is obtained for 
\begin_inset Formula $\nu\approx400$
\end_inset

, we have take this value to build the regularized classifier.
 This one makes a good result on the test set, with only 817 misclassified
 elements (85% accuracy).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename optimizationH12.pdf
	scale 50

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:tichonov_optimization"

\end_inset

Evolution of the risk (defined in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:section_tichonov"

\end_inset

) with the tichonov regularizer.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
Another improvement is to consider a logistic error : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{log}(W)=\frac{1}{2}\sum_{i=1}^{N}\mbox{lsexp}(y(x_{i}))-\tilde{t_{i}^{T}}y(x_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
There is no analytic solution of this kind of error.
 We are forced to implement a gradient descent.
 The course p.
 145 gives the value of the gradient : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{y_{i}}E_{i}=\sigma(y_{i})-\tilde{t_{i}}=[\frac{e^{y_{i}(k)}}{\sum_{k}e^{(k)}}]_{k=1...5}
\]

\end_inset


\end_layout

\begin_layout Standard
For early stopping, we have simply taken a validation set of 1/3 of the
 whole training set.
 Early stopping is triggered whenever the error on the validation does not
 decrease significantly (
\begin_inset Formula $error(epoch)>0.95*error(epoch-1)$
\end_inset

) The learning rate is 0.005.
 After launching the training of this classifier 10 times, we observe an
 average misclassification of 1009 elements on the test set (with a standard
 deviation of 55 elements).
 
\end_layout

\begin_layout Subsection
Multi-Class MLP 
\end_layout

\begin_layout Subsubsection
Support for multiple classes
\end_layout

\begin_layout Standard
There are several ways to extends our binary MLP to support multiple classes.
 We can do pairwise binary classification or K one-vs-all binary classifiers
 and then have one linear layer to make a decision based on that.
 We chose however another solution which is to extend the last layer to
 support multiple classes by setting its size to K=5.
 The target values are therefore in the form 
\begin_inset Formula $t_{i}=\text{[0,0,1,0,0]}$
\end_inset

 for class 
\begin_inset Formula $k=3$
\end_inset

 for example.
\end_layout

\begin_layout Subsubsection
Hyperparameters Search
\end_layout

\begin_layout Standard
Now that the MLP is working and the gradient has been tested, we need to
 find the optimal parameters -- namely 
\begin_inset Formula $H1,H2,\nu,\mu$
\end_inset

.
 We run two hyperparameters search, one for 
\begin_inset Formula $H1,H2$
\end_inset

 and one for 
\begin_inset Formula $\nu$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

.
 We assume naively that the sets of parameters are independent, altough
 we found that it is a reasonal assumption in practice.
 .
 The second search have been done on a single quadcore computer and took
 a couple of hours.
\end_layout

\begin_layout Paragraph
Number of neurons
\end_layout

\begin_layout Standard
The first search has been done on a small cluster of 4 nodes and 32 cores
 during 14 hours.
 The values for both H1 and H2 are 10,20,30,40,60,80,100.
 We also try two values of learning rates and momentum term to increase
 the confidence in the result.
 Moreover, we average the results for a single configuration over 3 runs.
 The results are shown in Figure ?.
\end_layout

\begin_layout Paragraph
Learning rate and Momentum
\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In general we achieve the main goal of the project which is to train an
 MLP for multiway classification of the NORB dataset.
 The project was helpful to teach us the subtlety and caveats of train neural
 networks.
\end_layout

\end_body
\end_document
