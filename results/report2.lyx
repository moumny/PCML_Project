#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 0.7in
\topmargin 1in
\rightmargin 0.7in
\bottommargin 1in
\columnsep 0.1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Mini-project Report
\end_layout

\begin_layout Author
Samuel Humeau, Benyounes Moumni
\end_layout

\begin_layout Abstract
The goal of this project is to implement and train a multilayer perceptron
 to classify images from the norb dataset.
 The dataset has XXXX images of object in 5 different categories: four-legged
 animals, human figures, airplanes, trucks, and cars.
 The Train set contains YYY samples while the test set contains XXX.
 In Section 2 we describe the implementations and result for the first task
 (binary classification.
 Section 3 and 4 describe the implementation of the mlp for multi way classifica
tion.
 We conclude in section 5.
\end_layout

\begin_layout Section
Binary Classification
\end_layout

\begin_layout Standard
The first task is to implement a binary classifier using a 2-hidden layer
 perceptron with logistic error.
 the architecture of the network is described in CITATION.
 We will first describe how we derived the gradient formulas then show the
 results.
 
\end_layout

\begin_layout Subsection
Implementing the Gradient Descent 
\end_layout

\begin_layout Paragraph
Third Layer
\end_layout

\begin_layout Standard
The goal is the minimize the logistic error 
\begin_inset Formula $E_{i}=log(1+e^{-t_{i}a^{(3)}})$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r^{(3)}=\frac{\partial E_{i}}{\partial a^{(3)}}=-t_{i}e^{-t_{i}a^{(3)}}\sigma(t_{i}a^{(3)})
\]

\end_inset

where 
\begin_inset Formula $\sigma(x)=\frac{1}{1+e^{-x}}$
\end_inset

 is the sigmoidal function.
 Using the formula from the book, the gradients in vectorized form are:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{W}^{(3)}}E_{i}=r^{(3)}\cdot\nabla_{\boldsymbol{W}^{(3)}}a^{(3)}=\boldsymbol{r}^{(3)}\cdot\left(\boldsymbol{z}^{(2)}\right)^{T}\\
\nabla_{\boldsymbol{b}^{(3)}}E_{i}=r^{(3)}\cdot\nabla_{\boldsymbol{b}^{(3)}}a^{(3)}=\boldsymbol{r}^{(3)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Second Layer
\end_layout

\begin_layout Standard
First, let's compute the derivatives of 
\begin_inset Formula $g_{2}(a_{L},a_{LR},a_{R})=a_{LR}\sigma(a_{L})\sigma(a_{R})$
\end_inset

.
 Remember that the derivative of the sigmoidal function is 
\begin_inset Formula $\sigma^{\prime}(x)=\sigma(x)(1-\sigma(x))$
\end_inset

.
 So,
\begin_inset Formula 
\[
\begin{cases}
\frac{\partial g_{2}}{\partial a_{LR}}=\sigma(a_{L})\sigma(a_{R})\\
\frac{\partial g_{2}}{\partial a_{L}}=g_{2}(a_{L},a_{LR},a_{R})(1-\sigma(a_{L}))\\
\frac{\partial g_{2}}{\partial a_{R}}=g_{2}(a_{L},a_{LR},a_{R})(1-\sigma(a_{R}))
\end{cases}
\]

\end_inset


\begin_inset Formula 
\[
\]

\end_inset

Now we derive the residual variables:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
r_{L,q}^{(2)} & = & \frac{\partial E_{i}}{\partial a_{L,q}^{(2)}}=\frac{\partial E_{i}}{\partial a^{(3)}}\cdot\frac{\partial a^{(3)}}{\partial a_{L,q}^{(2)}}\\
 & = & r^{(3)}\frac{\partial\mathbf{[\boldsymbol{(w}}^{(3)})^{T}\cdot\boldsymbol{g}_{2}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})+b^{(3)}]}{\partial a_{L,q}^{(2)}}\\
 & = & r^{(3)}w_{q}^{(3)}g_{2}^{\prime}(\boldsymbol{a}_{L,q}^{(2)},\boldsymbol{a}_{LR,q}^{(2)},\boldsymbol{a}_{R,q}^{(2)})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And finally in vectorized form we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\boldsymbol{r}_{L}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}\\
\boldsymbol{r}_{LR}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}\\
\boldsymbol{r}_{R}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
So now the gradients are:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{W}_{L}^{(2)}}E_{i}=\boldsymbol{r}_{L}^{(2)}\left(\nabla_{\boldsymbol{W}_{L}^{(2)}}a_{L}^{(2)}\right)^{T}=\boldsymbol{r}_{L}^{(2)}\cdot\left(\boldsymbol{z}_{L}^{(1)}\right)^{T}\\
\nabla_{\boldsymbol{W}_{LR}^{(2)}}E_{i}=\boldsymbol{r}_{LR}^{(2)}\left(\nabla_{\boldsymbol{W}_{LR}^{(2)}}a_{LR}^{(2)}\right)^{T}=\boldsymbol{r}_{LR}^{(2)}\cdot\begin{bmatrix}\boldsymbol{z}_{L}^{(1)}\\
\boldsymbol{z}_{R}^{(1)}
\end{bmatrix}^{T}\\
\nabla_{\boldsymbol{W}_{R}^{(2)}}E_{i}=\boldsymbol{r}_{R}^{(2)}\left(\nabla_{\boldsymbol{W}_{R}^{(2)}}a_{R}^{(2)}\right)^{T}=\boldsymbol{r}_{R}^{(2)}\cdot\left(\boldsymbol{z}_{R}^{(1)}\right)^{T}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
And for 
\begin_inset Formula $b$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{b}_{L}^{(2)}}E_{i}=\boldsymbol{r}_{L}^{(2)}\left(\nabla_{\boldsymbol{b}_{L}^{(2)}}a_{L}^{(2)}\right)^{T}=\boldsymbol{r}_{L}^{(2)}\\
\nabla_{\boldsymbol{b}_{LR}^{(2)}}E_{i}=\boldsymbol{r}_{LR}^{(2)}\left(\nabla_{\boldsymbol{b}_{LR}^{(2)}}a_{LR}^{(2)}\right)^{T}=\boldsymbol{r}_{LR}^{(2)}\\
\nabla_{\boldsymbol{b}_{R}^{(2)}}E_{i}=\boldsymbol{r}_{R}^{(2)}\left(\nabla_{\boldsymbol{b}_{R}^{(2)}}a_{R}^{(2)}\right)^{T}=\boldsymbol{r}_{R}^{(2)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Paragraph
First Layer
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{eqnarray*}
r_{L,q}^{(1)} & = & \frac{\partial E_{i}}{\partial a_{L,q}^{(1)}}=\sum_{j}^{h_{2}}\frac{\partial E_{i}}{\partial a_{L,j}^{(2)}}\cdot\frac{\partial a_{L,j}^{(2)}}{\partial a_{L,q}^{(1)}}+\sum_{j}^{h_{2}}\frac{\partial E_{i}}{\partial a_{LR,j}^{(2)}}\cdot\frac{\partial a_{LR,j}^{(2)}}{\partial a_{L,q}^{(1)}}\\
 & = & \sum_{j}^{h_{2}}r_{L,j}^{(2)}\frac{\partial a_{L,j}^{(2)}}{\partial a_{L,q}^{(1)}}+\sum_{j}^{h_{2}}r_{LR,j}^{(2)}\frac{\partial a_{LR,j}^{(2)}}{\partial a_{L,q}^{(1)}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{eqnarray*}
r_{R,q}^{(1)} & = & \frac{\partial E_{i}}{\partial a_{R,q}^{(1)}}=\sum_{j}^{h_{2}}\frac{\partial E_{i}}{\partial a_{R,j}^{(2)}}\cdot\frac{\partial a_{R,j}^{(2)}}{\partial a_{R,q}^{(1)}}+\sum_{j}^{h_{2}}\frac{\partial E_{i}}{\partial a_{LR,j}^{(2)}}\cdot\frac{\partial a_{LR,j}^{(2)}}{\partial a_{R,q}^{(1)}}\\
 & = & \sum_{j}^{h_{2}}r_{R,j}^{(2)}\frac{\partial a_{R,j}^{(2)}}{\partial a_{R,q}^{(1)}}+\sum_{j}^{h_{2}}r_{LR,j}^{(2)}\frac{\partial a_{LR,j}^{(2)}}{\partial a_{R,q}^{(1)}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So, for a 
\begin_inset Formula $(p,j)$
\end_inset

 pair, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial a_{L,j}^{(2)}}{\partial a_{L,q}^{(1)}} & = & \frac{\partial\left[\mathbf{\boldsymbol{w}}_{L,j}^{(2)}\cdot\boldsymbol{g}_{1}(\boldsymbol{a}_{L}^{(1)})+b_{L,j}^{(2)}\right]}{\partial a_{L,q}^{(1)}}\\
 & = & \frac{\partial\left[\sum_{k}w_{L,j,k}^{(2)}\cdot g_{1}(a_{L,k}^{(1)})\right]}{\partial a_{L,q}^{(1)}}\\
 & = & \frac{\partial\left[w_{L,j,q}^{(2)}\cdot g_{1}(a_{L,q}^{(1)})\right]}{\partial a_{L,q}^{(1)}}\\
 & = & w_{L,j,q}^{(2)}\cdot g_{1}^{\prime}(a_{L,q}^{(1)})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial a_{LR,j}^{(2)}}{\partial a_{L,q}^{(1)}} & = & \frac{\partial\left[\mathbf{\boldsymbol{w}}_{LR,j}^{(2)}\cdot\begin{bmatrix}\boldsymbol{g}_{1}(\boldsymbol{a}_{L}^{(1)})\\
\boldsymbol{g}_{1}(\boldsymbol{a}_{R}^{(1)})
\end{bmatrix}+b_{L,j}^{(2)}\right]}{\partial a_{L,q}^{(1)}}\\
 & = & \frac{\partial\left[\sum_{k=1}^{h1}w_{LR,j}^{(2)}\left(k\right)\cdot g_{1}(a_{L,k}^{(1)})+\sum_{k=1}^{h1}w_{LR,j}^{(2)}\left(h_{1}+k\right)\cdot g_{1}(a_{R,k}^{(1)})\right]}{\partial a_{L,q}^{(1)}}\\
 & = & w_{LR,j}^{(2)}\left(q\right)\cdot g_{1}^{\prime}(a_{L,q}^{(1)})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
r_{L,q}^{(1)} & = & \sum_{j}^{h_{2}}r_{L,j}^{(2)}\frac{\partial a_{L,j}^{(2)}}{\partial a_{L,q}^{(1)}}+\sum_{j}^{h_{2}}r_{LR,j}^{(2)}\frac{\partial a_{LR,j}^{(2)}}{\partial a_{L,q}^{(1)}}\\
 & = & \sum_{j}^{h_{2}}r_{L,j}^{(2)}\cdot w_{L,j,q}^{(2)}\cdot g_{1}^{\prime}(a_{L,q}^{(1)})+\sum_{j}^{h_{2}}r_{LR,j}^{(2)}\cdot w_{LR,j}^{(2)}\left(q\right)\cdot g_{1}^{\prime}(a_{L,q}^{(1)})\\
 & = & g_{1}^{\prime}(a_{L,q}^{(1)})\cdot\left(\boldsymbol{W}_{L}^{(2)}\right)_{q}^{T}\cdot\boldsymbol{r}_{L}^{(2)}+g_{1}^{\prime}(a_{L,q}^{(1)})\cdot\left(\boldsymbol{W}_{LR}^{(2)}\right)_{q}^{T}\cdot\boldsymbol{r}_{LR}^{(2)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In summary after vectorization we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\boldsymbol{r}_{L}^{(1)} & =diag\left(g_{1}^{\prime}(\boldsymbol{a}_{L}^{(1)})\right)\cdot\left(\boldsymbol{W}_{L}^{(2)}\right)^{T}\cdot\boldsymbol{r}_{L}^{(2)}+diag\left(g_{1}^{\prime}(\boldsymbol{a}_{L}^{(1)})\right)\cdot\left(\boldsymbol{W}_{LR}^{(2)}(:,1:h_{1})\right)^{T}\cdot\boldsymbol{r}_{LR}^{(2)}\\
\boldsymbol{r}_{R}^{(1)} & =diag\left(g_{1}^{\prime}(\boldsymbol{a}_{R}^{(1)})\right)\cdot\left(\boldsymbol{W}_{R}^{(2)}\right)^{T}\cdot\boldsymbol{r}_{R}^{(2)}+diag\left(g_{1}^{\prime}(\boldsymbol{a}_{R}^{(1)})\right)\cdot\left(\boldsymbol{W}_{LR}^{(2)}(:,h_{1}+1:2\times h_{1})\right)^{T}\cdot\boldsymbol{r}_{LR}^{(2)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Now that we have computed the residual variables, we can easily find the
 gradients for the first layer.
 Indeed,
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{W}_{L}^{(1)}}E_{i}=\boldsymbol{r}_{L}^{(1)}\cdot\left(\nabla_{\boldsymbol{W}_{L}^{(1)}}a_{L}^{(1)}\right)^{T}=\boldsymbol{r}_{L}^{(1)}\cdot\left(\boldsymbol{x}_{L}\right)^{T}\\
\nabla_{\boldsymbol{W}_{R}^{(1)}}E_{i}=\boldsymbol{r}_{R}^{(1)}\cdot\left(\nabla_{\boldsymbol{W}_{R}^{(1)}}a_{R}^{(1)}\right)^{T}=\boldsymbol{r}_{R}^{(1)}\cdot\left(\boldsymbol{x}_{R}\right)^{T}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
And,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{b}_{L}^{(1)}}E_{i}=\boldsymbol{r}_{L}^{(1)}\cdot\left(\nabla_{\boldsymbol{b}_{L}^{(1)}}a_{L}^{(1)}\right)^{T}=\boldsymbol{r}_{L}^{(1)}\\
\nabla_{\boldsymbol{b}_{R}^{(1)}}E_{i}=\boldsymbol{r}_{R}^{(1)}\cdot\left(\nabla_{\boldsymbol{b}_{R}^{(1)}}a_{R}^{(1)}\right)^{T}=\boldsymbol{r}_{R}^{(1)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Subsection
Results 
\end_layout

\begin_layout Standard
We tried the classifier with several configuration of number of layers and
 learning rate.
 The classification in all cases is perfect after the first epoch with a
 very low error.
 The reason for such a good performance on the validation set is that the
 training set is very well separated.
 In this case there is not much optimization possible, as the difference
 in error in the validation is insignificant to make a decision.
\end_layout

\begin_layout Subsubsection
Extrapolation ?
\end_layout

\begin_layout Standard
So to check the algorithm beyond testing the gradient we made another dataset
 from the 5class one.
 The idea is that some other classes are less separable and would allow
 us to have a better idea of the performance of our implementation.
\end_layout

\begin_layout Section
Multi-way classification
\end_layout

\begin_layout Standard
To be able to compare the multiway MLP were going to implement, it is important
 to build other solutions to compare.
 Indeed it is expected that a MLP outperforms all linear solutions.
 Therefore we have implemented 3 linear classifiers described in the next
 three sections.
 
\end_layout

\begin_layout Subsection
Linear regression with squared error
\end_layout

\begin_layout Standard
For this part, each datapoint belongs to one of 5 class.
 Thus, the label attributed for each datapoint is no more a scalar but a
 binary 5-dimensional vector.
 For example if 
\begin_inset Formula $x$
\end_inset

 belongs to the class number 2, its label will be 
\begin_inset Formula $\widetilde{t_{i}}=[0,1,0,0,0]$
\end_inset

.
 Let consider the classification error on the training set to be : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{2}(W)=\sum_{i=1}^{N}\left\Vert y_{i}-\tilde{t}_{i}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The linear classifier computes the 5-dimensional output : 
\begin_inset Formula $y_{i}=W[x_{i};1]$
\end_inset

 (we had a constant coordinate in each data points in order to incorporate
 the bias in the matrix W).
 Then the class is determined by 
\begin_inset Formula $argmax_{j}y_{i}(j)$
\end_inset

.
 In the case of the squared error above, the course shows that the matrix
 W that optimizes the regression on the training set is given by : 
\end_layout

\begin_layout Standard
\begin_inset Formula $W_{optimal}=(\Phi^{T}\Phi)^{-1}\Phi T$
\end_inset

 where 
\begin_inset Formula $\Phi$
\end_inset

is a matrix where each column is a datapoint, and 
\begin_inset Formula $T$
\end_inset

 is the the matrix that regroup the corresponding labels.
 
\end_layout

\begin_layout Standard
By computing this classifier, we obtain 1228 misclassified elements on the
 test set (over a total of 5400 data points), which means an accuracy of
 77%.
 
\end_layout

\begin_layout Subsection
Squared error with Tichonov regularizer
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:section_tichonov"

\end_inset


\end_layout

\begin_layout Standard
A classic improvement of linear regression is to constrain the weights using
 a tichonov regularizer.
 The error is now : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{2}(W)=\frac{1}{2}\sum_{i=1}^{N}\left\Vert y_{i}-\tilde{t}_{i}\right\Vert ^{2}+\frac{\nu}{2}\left\Vert W\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
In this case, for a fixed 
\begin_inset Formula $\nu$
\end_inset

, the course gives us the optimal weights for the regression on the training
 set (using the same notation introduced above) :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{tichonov}=(\Phi^{T}\Phi+\nu I)^{-1}\Phi^{T}T
\]

\end_inset

In order to find the best regularizer 
\begin_inset Formula $\nu$
\end_inset

, we compute a 10-folds cross validation on the training set, for different
 values of 
\begin_inset Formula $\nu$
\end_inset

.
 We have selected the regularizer that minimizes the risk, which is computed
 as follow : after splitting the training set in 10 equals parts, 
\begin_inset Formula $risk=\frac{1}{10}\sum_{m=1}^{10}R^{(-m)}(W^{(-m)})$
\end_inset

, where 
\begin_inset Formula $R^{(-m)}$
\end_inset

is the error 
\begin_inset Formula $E_{2}$
\end_inset

computed only of the 
\begin_inset Formula $m^{th}$
\end_inset

part of the classifier trained with the 9 other parts of the training set.
 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tichonov_optimization"

\end_inset

 shows the evolution of the risk with 
\begin_inset Formula $\nu$
\end_inset

.
 Since minimum is obtained for 
\begin_inset Formula $\nu\approx400$
\end_inset

, we have take this value to build the regularized classifier.
 This one makes a good result on the test set, with only 817 misclassified
 elements (85% accuracy).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:tichonov_optimization"

\end_inset

Evolution of the risk (defined in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:section_tichonov"

\end_inset

) with the tichonov regularizer.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
Another improvement is to consider a logistic error : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{log}(W)=\frac{1}{2}\sum_{i=1}^{N}\mbox{lsexp}(y(x_{i}))-\tilde{t_{i}^{T}}y(x_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
There is no analytic solution of this kind of error.
 We are forced to implement a gradient descent.
 The course p.
 145 gives the value of the gradient : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{y_{i}}E_{i}=\sigma(y_{i})-\tilde{t_{i}}=[\frac{e^{y_{i}(k)}}{\sum_{k}e^{(k)}}]_{k=1...5}
\]

\end_inset


\end_layout

\begin_layout Standard
For early stopping, we have simply taken a validation set of 1/3 of the
 whole training set.
 Early stopping is triggered whenever the error on the validation does not
 decrease significantly (
\begin_inset Formula $error(epoch)>0.95*error(epoch-1)$
\end_inset

) The learning rate is 0.005.
 After launching the training of this classifier 10 times, we observe an
 average misclassification of 1009 elements on the test set (with a standard
 deviation of 55 elements).
 
\end_layout

\begin_layout Subsection
Extension to Multi-Class MLP 
\end_layout

\begin_layout Standard
There are several ways to extends our binary MLP to support multiple classes.
 We can do pairwise binary classification or K one-vs-all binary classifiers
 and then have one linear layer to make a decision based on that.
 We chose however another solution which is to extend the last layer to
 support multiple classes by setting its size to K=5.
 The target values are therefore of the form 
\begin_inset Formula $t_{i}=\text{[0,0,1,0,0]}$
\end_inset

 for class 
\begin_inset Formula $k=3$
\end_inset

 
\end_layout

\begin_layout Subsection
Computation of gradient 
\end_layout

\begin_layout Subsubsection
Forward pass
\end_layout

\begin_layout Standard
The first and second layer or the forward bass are exactly the same as in
 the binary case.
 Only change the third layer : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a_{q}^{(3)}=\mathbf{\boldsymbol{(w}}_{q}^{(3)})^{T}\cdot\boldsymbol{z}^{(2)}+b_{q}^{(3)},\; q=1,...,K
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{a}^{(3)}=\mathbf{W}^{(3)}\cdot\boldsymbol{z}^{(2)}+\boldsymbol{b}^{(3)}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Backward pass
\end_layout

\begin_layout Paragraph
Third Layer
\end_layout

\begin_layout Standard
The goal is the minimize 
\begin_inset Formula $E_{2}(\boldsymbol{w})=\frac{1}{2}\sum_{i=1}^{N}\left\Vert \boldsymbol{a}^{(3)}(\boldsymbol{x}_{i})-\tilde{\boldsymbol{t}_{i}}\right\Vert ^{2}$
\end_inset

.
 We compute the gradient for one 
\begin_inset Formula $i$
\end_inset

 that is we minimize 
\begin_inset Formula $E_{2,i}$
\end_inset

 that we will refer to as 
\begin_inset Formula $E_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r_{q}^{(3)}=\frac{\partial E_{i}}{\partial a_{q}^{(3)}}=2(a_{q}^{(3)}-\tilde{\boldsymbol{t}_{q}}),\; q=1,...,K
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{r}^{(3)}=[r_{q}^{(3)}]=2(\boldsymbol{a}^{(3)}-\tilde{\boldsymbol{t}})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{W}^{(3)}}E_{i}=r^{(3)}\cdot\nabla_{\boldsymbol{W}^{(3)}}a^{(3)}=\boldsymbol{r}^{(3)}\cdot\left(\boldsymbol{z}^{(2)}\right)^{T}\\
\nabla_{\boldsymbol{b}^{(3)}}E_{i}=r^{(3)}\cdot\nabla_{\boldsymbol{b}^{(3)}}a^{(3)}=\boldsymbol{r}^{(3)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Second Layer
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
r_{L,q}^{(2)} & = & \frac{\partial E_{i}}{\partial a_{L,q}^{(2)}}=\sum_{j}^{K}\frac{\partial E_{i}}{\partial a_{j}^{(3)}}\cdot\frac{\partial a_{j}^{(3)}}{\partial a_{L,q}^{(2)}}\\
 & = & \sum_{j}^{K}r_{j}^{(3)}\frac{\partial\mathbf{[\boldsymbol{(w}}_{j}^{(3)})^{T}\cdot\boldsymbol{g}_{2}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})+b_{j}^{(3)}]}{\partial a_{L,q}^{(2)}}\\
 & = & \sum_{j}^{K}r_{j}^{(3)}\frac{\partial\mathbf{[\sum_{k}\boldsymbol{w}}_{j,k}^{(3)}g_{2}(\boldsymbol{a}_{L,k}^{(2)},\boldsymbol{a}_{LR,k}^{(2)},\boldsymbol{a}_{R,k}^{(2)})]}{\partial a_{L,q}^{(2)}}\\
 & = & \sum_{j}^{K}r_{j}^{(3)}\frac{\partial\mathbf{[w}_{j,q}^{(3)}g_{2}(\boldsymbol{a}_{L,q}^{(2)},\boldsymbol{a}_{LR,q}^{(2)},\boldsymbol{a}_{R,q}^{(2)})]}{\partial a_{L,q}^{(2)}}\\
 & = & \sum_{j}^{K}r_{j}^{(3)}w_{j,q}^{(3)}g_{2}^{\prime}(\boldsymbol{a}_{L,q}^{(2)},\boldsymbol{a}_{LR,q}^{(2)},\boldsymbol{a}_{R,q}^{(2)})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Vectorizing gives:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\boldsymbol{r}_{L}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}\\
\boldsymbol{r}_{LR}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}\\
\boldsymbol{r}_{R}^{(2)}=\left(diag\left(g_{2}^{\prime}(\boldsymbol{a}_{L}^{(2)},\boldsymbol{a}_{LR}^{(2)},\boldsymbol{a}_{R}^{(2)})\right)\right)\cdot(\boldsymbol{W}^{(3)})^{T}\cdot\boldsymbol{r}^{(3)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
So now the gradients are:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{W}_{L}^{(2)}}E_{i}=\boldsymbol{r}_{L}^{(2)}\left(\nabla_{\boldsymbol{W}_{L}^{(2)}}a_{L}^{(2)}\right)^{T}=\boldsymbol{r}_{L}^{(2)}\cdot\left(\boldsymbol{z}_{L}^{(1)}\right)^{T}\\
\nabla_{\boldsymbol{W}_{LR}^{(2)}}E_{i}=\boldsymbol{r}_{LR}^{(2)}\left(\nabla_{\boldsymbol{W}_{LR}^{(2)}}a_{LR}^{(2)}\right)^{T}=\boldsymbol{r}_{LR}^{(2)}\cdot\begin{bmatrix}\boldsymbol{z}_{L}^{(1)}\\
\boldsymbol{z}_{R}^{(1)}
\end{bmatrix}^{T}\\
\nabla_{\boldsymbol{W}_{R}^{(2)}}E_{i}=\boldsymbol{r}_{R}^{(2)}\left(\nabla_{\boldsymbol{W}_{R}^{(2)}}a_{R}^{(2)}\right)^{T}=\boldsymbol{r}_{R}^{(2)}\cdot\left(\boldsymbol{z}_{R}^{(1)}\right)^{T}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
And for 
\begin_inset Formula $b$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\begin{cases}
\nabla_{\boldsymbol{b}_{L}^{(2)}}E_{i}=\boldsymbol{r}_{L}^{(2)}\left(\nabla_{\boldsymbol{b}_{L}^{(2)}}a_{L}^{(2)}\right)^{T}=\boldsymbol{r}_{L}^{(2)}\\
\nabla_{\boldsymbol{b}_{LR}^{(2)}}E_{i}=\boldsymbol{r}_{LR}^{(2)}\left(\nabla_{\boldsymbol{b}_{LR}^{(2)}}a_{LR}^{(2)}\right)^{T}=\boldsymbol{r}_{LR}^{(2)}\\
\nabla_{\boldsymbol{b}_{R}^{(2)}}E_{i}=\boldsymbol{r}_{R}^{(2)}\left(\nabla_{\boldsymbol{b}_{R}^{(2)}}a_{R}^{(2)}\right)^{T}=\boldsymbol{r}_{R}^{(2)}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Paragraph
First Layer
\end_layout

\begin_layout Standard
The first layer is exactly the same as in the binary case.
\end_layout

\begin_layout Subsection
Hyperparameters Search
\end_layout

\begin_layout Standard
Now tha
\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In general we achieve the main goal of the project which is to train an
 MLP for multiway classification of the NORB dataset.
 The project was helpful to teach us the subtlety and caveats of train neural
 networks.
\end_layout

\end_body
\end_document
